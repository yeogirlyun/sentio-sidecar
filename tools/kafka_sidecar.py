#!/usr/bin/env python3
"""
Kafka sidecar producer for Sentio Lite (SIGOR v2.0)

Publishes minute-by-minute:
 - Prices per symbol (sentio.prices.minute.v1)
 - Positions state (sentio.positions.state.v1, compacted)
 - Portfolio metrics (sentio.portfolio.minute.v1, compacted)
 - Trades as they occur (sentio.trades.executed.v1)

Modes:
- replay: Reads from results.json (embedded filtered price_data) + trades.jsonl and replays a day.
- polygon: Streams live minute bars from Polygon and publishes price + heartbeat (engine live wiring to follow).

Environment variables (or CLI flags):
 - KAFKA_BOOTSTRAP_SERVERS
 - TOPIC_PREFIX (default: sentio.)
 - REPLAY_SPEED_MS (default: 60) â†’ 1 market minute per 60ms
 - RESULTS_PATH (default: results.json)
 - TRADES_PATH (default: trades.jsonl)
 - STRATEGY (default: SIGOR)
 - ENV (default: MOCK)
 - RUN_ID (default: autogenerated UUID4)

Note: Signal messages are intentionally omitted in v1 to avoid
duplicating strategy internals. They can be added once the engine
exposes per-bar signals in results.json.
"""

import argparse
import json
import os
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, List, Any

from confluent_kafka import Producer


# Helper to compute intraday barId (1..391) from an ET ISO timestamp
def bar_id_from_ts_iso(ts_iso: str) -> int:
    try:
        hh = int(ts_iso[11:13])
        mm = int(ts_iso[14:16])
        minute_of_day = hh * 60 + mm
        start = 9 * 60 + 30
        bid = (minute_of_day - start) + 1
        if bid < 1:
            return 1
        if bid > 391:
            return 391
        return bid
    except Exception:
        return 0

def load_symbols_from_conf(conf_path: str = "config/symbols.conf") -> List[str]:
    symbols: List[str] = []
    try:
        with open(conf_path, "r") as f:
            for raw in f:
                line = raw.strip()
                if not line or line.startswith('#'):
                    continue
                symbols.append(line.upper())
    except Exception:
        return []
    return symbols

try:
    from polygon import WebSocketClient
    from polygon.websocket.models import WebSocketMessage
except Exception:
    WebSocketClient = None  # Lazy optional import for replay users


def iso_et_from_ms(ms: int) -> str:
    """Convert epoch milliseconds to ET ISO string with proper timezone"""
    from zoneinfo import ZoneInfo
    # Convert epoch ms to datetime in ET timezone
    dt_utc = datetime.fromtimestamp(ms / 1000, tz=timezone.utc)
    dt_et = dt_utc.astimezone(ZoneInfo("America/New_York"))
    # Return in ISO format with ET offset
    return dt_et.strftime("%Y-%m-%dT%H:%M:%S%z")


def load_results(path: str) -> Dict[str, Any]:
    with open(path, "r") as f:
        return json.load(f)


def load_golden_db(path: str) -> Dict[str, Any]:
    if not os.path.exists(path):
        return {}
    with open(path, "r") as f:
        return json.load(f)


def load_trades_jsonl(path: str, results_path: str = None, filter_start_ms: int = None) -> List[Dict[str, Any]]:
    """Load trades from trades.jsonl or extract from results.json if jsonl doesn't exist

    Args:
        path: Path to trades.jsonl file
        results_path: Path to results.json as fallback
        filter_start_ms: If provided, only include trades with timestamp >= filter_start_ms
    """
    trades: List[Dict[str, Any]] = []

    # Try loading from trades.jsonl first
    if os.path.exists(path):
        with open(path, "r") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                trades.append(json.loads(line))
        trades.sort(key=lambda t: t.get("timestamp_ms", 0))
        # Apply filter if provided
        if filter_start_ms is not None:
            trades = [t for t in trades if t.get("timestamp_ms", 0) >= filter_start_ms]
        return trades

    # Fallback: extract trades from results.json
    if results_path and os.path.exists(results_path):
        print(f"[kafka_sidecar] trades.jsonl not found, extracting trades from {results_path}", flush=True)
        with open(results_path, "r") as f:
            results = json.load(f)
            results_trades = results.get("trades", [])

            # Convert results.json trade format to expected format
            for rt in results_trades:
                # Entry trade
                entry_trade = {
                    "symbol": rt.get("symbol"),
                    "action": "ENTRY",
                    "timestamp_ms": rt.get("entry_time_ms"),
                    "bar_id": rt.get("entry_bar_id"),
                    "price": rt.get("entry_price"),
                    "shares": rt.get("shares"),
                    "value": rt.get("entry_price", 0) * rt.get("shares", 0),
                    "pnl": 0,
                    "pnl_pct": 0,
                    "bars_held": 0,
                    "reason": "Entry"
                }
                trades.append(entry_trade)

                # Exit trade
                # Calculate bars_held from timestamps (each bar = 1 minute = 60000ms)
                entry_time_ms = rt.get("entry_time_ms", 0)
                exit_time_ms = rt.get("exit_time_ms", 0)
                bars_held = int((exit_time_ms - entry_time_ms) / 60000) if entry_time_ms and exit_time_ms else 0

                exit_trade = {
                    "symbol": rt.get("symbol"),
                    "action": "EXIT",
                    "timestamp_ms": rt.get("exit_time_ms"),
                    "bar_id": rt.get("exit_bar_id"),
                    "price": rt.get("exit_price"),
                    "shares": rt.get("shares"),
                    "value": rt.get("exit_price", 0) * rt.get("shares", 0),
                    "pnl": rt.get("pnl", 0),
                    "pnl_pct": rt.get("pnl_pct", 0) * 100,  # Convert to percentage
                    "bars_held": bars_held,  # Calculated from time difference
                    "reason": "Exit"
                }
                trades.append(exit_trade)

            trades.sort(key=lambda t: t.get("timestamp_ms", 0))
            print(f"[kafka_sidecar] Extracted {len(trades)} trade events from results.json", flush=True)

            # Apply filter if provided
            if filter_start_ms is not None:
                trades_before = len(trades)
                trades = [t for t in trades if t.get("timestamp_ms", 0) >= filter_start_ms]
                print(f"[kafka_sidecar] Filtered to {len(trades)} trade events (removed {trades_before - len(trades)} warmup trades)", flush=True)

    return trades


def build_kafka_producer(bootstrap: str) -> Producer:
    conf = {
        "bootstrap.servers": bootstrap,
        "linger.ms": 5,
        "batch.num.messages": 1000,
        "acks": "1",  # Wait for leader acknowledgment
        "request.timeout.ms": 30000,  # 30 second timeout
        "message.timeout.ms": 30000,  # Message delivery timeout
    }
    return Producer(conf)


def delivery_report(err, msg):
    """Delivery callback called once for each message produced"""
    if err is not None:
        print(f'Message delivery failed: {err}', flush=True)
    # Success - no print to keep output clean

def publish(producer: Producer, topic: str, key: str, value: Dict[str, Any], headers: Dict[str, str]):
    headers_list = [(k, v.encode("utf-8")) for k, v in headers.items()]
    try:
        producer.produce(
            topic,
            key=key.encode("utf-8"),
            value=json.dumps(value).encode("utf-8"),
            headers=headers_list,
            callback=delivery_report
        )
    except Exception as e:
        print(f'Failed to produce message to {topic}: {e}', flush=True)


def main():
    parser = argparse.ArgumentParser(description="Sentio Lite Kafka sidecar producer")
    parser.add_argument("--bootstrap", default=os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092"))
    parser.add_argument("--prefix", default=os.getenv("TOPIC_PREFIX", "sentio."))
    parser.add_argument("--results", default=os.getenv("RESULTS_PATH", "results.json"))
    parser.add_argument("--golden", default=os.getenv("GOLDEN_DB_PATH", "golden_db.json"))
    parser.add_argument("--trades", default=os.getenv("TRADES_PATH", "trades.jsonl"))
    parser.add_argument("--speed-ms", type=int, default=int(os.getenv("REPLAY_SPEED_MS", "60")))
    parser.add_argument("--strategy", default=os.getenv("STRATEGY", "SIGOR"))
    parser.add_argument("--env", dest="env_name", default=os.getenv("ENV", "MOCK"))
    parser.add_argument("--run-id", default=os.getenv("RUN_ID", str(uuid.uuid4())))
    parser.add_argument("--mode", choices=["replay", "polygon"], default=os.getenv("PRODUCER_MODE", "replay"))
    parser.add_argument("--polygon-key", default=os.getenv("POLYGON_API_KEY", ""))
    args = parser.parse_args()
    if args.mode == "replay":
        golden = load_golden_db(args.golden)
        use_golden = bool(golden)
        if use_golden:
            print(f"[kafka_sidecar] Using Golden DB: {args.golden}", flush=True)
        results = load_results(args.results)

        # Extract symbols, price data, and bar data
        if use_golden:
            symbols = golden.get("symbols", [])
            golden_bars: List[Dict[str, Any]] = golden.get("bars", [])
        else:
            symbols_str = results.get("metadata", {}).get("symbols", "")
            symbols = [s.strip() for s in symbols_str.split(",") if s.strip()]
        price_data: Dict[str, List[Dict[str, Any]]] = results.get("price_data", {})
        bar_data: List[Dict[str, Any]] = results.get("bar_data", [])

        # Get test date to filter warmup bars
        test_date_str = os.getenv("TEST_DATE", "")
        test_date_start_ms = None
        if test_date_str:
            try:
                import zoneinfo
                et = zoneinfo.ZoneInfo("America/New_York")
                test_date_dt = datetime.strptime(test_date_str, "%Y-%m-%d").replace(tzinfo=et)
                # Market opens at 9:30 AM ET
                test_date_start_ms = int(test_date_dt.replace(hour=9, minute=30, second=0).timestamp() * 1000)
                print(f"[kafka_sidecar] Filtering to test date {test_date_str} (>= {iso_et_from_ms(test_date_start_ms)})", flush=True)
            except Exception as e:
                print(f"[kafka_sidecar] Failed to parse TEST_DATE: {e}", flush=True)

        # Load trades with filtering applied (fallback to results.json)
        trades = load_trades_jsonl(args.trades, args.results, filter_start_ms=test_date_start_ms)
        # Also map trades from Golden DB if available (entry/exit events)
        golden_trades_events: List[Dict[str, Any]] = []
        if use_golden:
            for gt in golden.get("trades", []):
                sym = gt.get("symbol")
                entry_ms = gt.get("entryMs")
                exit_ms = gt.get("exitMs")
                shares = gt.get("shares", 0)
                entry_price = gt.get("entryPrice", 0.0)
                exit_price = gt.get("exitPrice", 0.0)
                if entry_ms:
                    golden_trades_events.append({
                        "symbol": sym,
                        "action": "ENTRY",
                        "timestamp_ms": entry_ms,
                        "bar_id": gt.get("entryBarId"),
                        "price": entry_price,
                        "shares": shares,
                        "value": entry_price * shares,
                        "pnl": 0.0,
                        "pnl_pct": 0.0,
                        "bars_held": 0,
                        "reason": "Entry",
                        # Pass through engine-provided event annotation if present
                        "eventAnnotation": gt.get("entryAnnotation") or gt.get("eventAnnotation") or ""
                    })
                if exit_ms:
                    # Compute pnl_pct as percentage
                    pnl = (exit_price - entry_price) * shares
                    pnl_pct = ((exit_price - entry_price) / entry_price * 100.0) if entry_price else 0.0
                    bars_held = int((exit_ms - entry_ms) / 60000) if entry_ms and exit_ms else 0
                    golden_trades_events.append({
                        "symbol": sym,
                        "action": "EXIT",
                        "timestamp_ms": exit_ms,
                        "bar_id": gt.get("exitBarId"),
                        "price": exit_price,
                        "shares": shares,
                        "value": exit_price * shares,
                        "pnl": pnl,
                        "pnl_pct": pnl_pct,
                        "bars_held": bars_held,
                        "reason": "Exit",
                        # Pass through engine-provided event annotation if present
                        "eventAnnotation": gt.get("exitAnnotation") or gt.get("eventAnnotation") or ""
                    })
            # Merge and sort with de-duplication.
            # If trades.jsonl provided entries, prefer those and only add missing ones from Golden DB.
            if golden_trades_events:
                if trades:
                    existing = set((t.get("symbol"), t.get("action"), t.get("timestamp_ms")) for t in trades)
                    for ge in golden_trades_events:
                        key = (ge.get("symbol"), ge.get("action"), ge.get("timestamp_ms"))
                        if key not in existing:
                            trades.append(ge)
                else:
                    trades.extend(golden_trades_events)
                trades.sort(key=lambda t: t.get("timestamp_ms", 0))

        # Prepare bar stream
        if use_golden:
            # Golden bars have tsET; filter by ET date/time to avoid timezone ambiguities
            if test_date_str:
                def is_test_bar(b: Dict[str, Any]) -> bool:
                    ts = b.get("tsET", "")
                    if not ts or len(ts) < 19:
                        return True  # If missing tsET, don't drop the bar
                    date = ts[:10]
                    time = ts[11:19]
                    return date == test_date_str and time >= "09:30:00"
                pre = len(golden_bars)
                filtered_bar_data = [b for b in golden_bars if is_test_bar(b)]
                print(f"[kafka_sidecar] GoldenDB: Filtered to {len(filtered_bar_data)} bars (removed {pre - len(filtered_bar_data)} pre-market/warmup bars by ET time)", flush=True)
            else:
                filtered_bar_data = golden_bars
                print(f"[kafka_sidecar] GoldenDB: Streaming {len(filtered_bar_data)} bars", flush=True)
        else:
            # Filter bar_data to only include test date bars (remove warmup)
            if test_date_str:
                def is_test_bar_json(b: Dict[str, Any]) -> bool:
                    ts = b.get("timestamp_et") or b.get("tsET") or ""
                    if not ts or len(ts) < 19:
                        return True
                    date = ts[:10]
                    time = ts[11:19]
                    return date == test_date_str and time >= "09:30:00"
                pre = len(bar_data)
                filtered_bar_data = [bar for bar in bar_data if is_test_bar_json(bar)]
                print(f"[kafka_sidecar] Filtered to {len(filtered_bar_data)} bars (removed {pre - len(filtered_bar_data)} warmup bars by ET time)", flush=True)
            else:
                filtered_bar_data = bar_data
                print(f"[kafka_sidecar] Streaming {len(filtered_bar_data)} bars from bar_data", flush=True)

        producer = build_kafka_producer(args.bootstrap)
        headers = {"strategy": args.strategy, "env": args.env_name, "runId": args.run_id, "testDate": test_date_str}

        # Group trades by timestamp for publishing
        trades_by_timestamp: Dict[int, List[Dict[str, Any]]] = {}
        for t in trades:
            ts = t.get("timestamp_ms", 0)
            trades_by_timestamp.setdefault(ts, []).append(t)

        print(f"[kafka_sidecar] Total trades: {len(trades)}, Timestamps: {len(trades_by_timestamp)}", flush=True)

        # Build price lookup for price publishing (results.json path only)
        price_lookup: Dict[str, Dict[int, Dict[str, Any]]] = {}
        if not use_golden:
            for sym in symbols:
                bucketed: Dict[int, Dict[str, Any]] = {}
                for bar in price_data.get(sym, []):
                    ts = bar.get("timestamp_ms") or bar.get("timestamp")
                    if isinstance(ts, str) and ts.isdigit():
                        ts = int(ts)
                    if not isinstance(ts, (int, float)):
                        continue
                    minute = int(ts // 60000 * 60000)
                    bucketed[minute] = bar
                price_lookup[sym] = bucketed

        prices_topic = f"{args.prefix}prices.minute.v1"
        trades_topic = f"{args.prefix}trades.executed.v1"
        pos_topic = f"{args.prefix}positions.state.v1"
        port_topic = f"{args.prefix}portfolio.minute.v1"
        hb_topic = f"{args.prefix}heartbeat.v1"

        print(f"[kafka_sidecar] Starting replay: {len(filtered_bar_data)} bars at {args.speed_ms}ms/bar", flush=True)

        # Track previous bar positions to detect closes
        prev_positions = set()
        position_entry_times = {}  # Track when each position was opened (symbol -> timestamp_ms)

        for i, bar_state in enumerate(filtered_bar_data):
            if use_golden:
                timestamp_ms = bar_state.get("tsMs")
                ts_iso = bar_state.get("tsET") or iso_et_from_ms(timestamp_ms)
                # Publish prices for all symbols from symbolBars
                signal_map: Dict[str, Dict[str, Any]] = {}
                for ss in bar_state.get("symbolSignals", []):
                    signal_map[ss.get("symbol")] = ss
                # Build annotation map if present
                ann_map: Dict[str, str] = {}
                for sa in bar_state.get("symbolAnnotations", []):
                    ann_map[sa.get("symbol")] = sa.get("text", "")
                for sb in bar_state.get("symbolBars", []):
                    sym = sb.get("symbol")
                    sig = signal_map.get(sym, {})
                    ann = ann_map.get(sym, "")
                    bar_id = bar_state.get("barId") or bar_id_from_ts_iso(ts_iso)
                    payload = {
                        "symbol": sym,
                        "tsET": ts_iso,
                        "barId": bar_id,
                        "open": sb.get("open"),
                        "high": sb.get("high"),
                        "low": sb.get("low"),
                        "close": sb.get("close"),
                        "volume": sb.get("volume"),
                        "signal": {
                            "probability": sig.get("probability"),
                            "confidence": sig.get("confidence"),
                            "detectors": {
                                "boll": sig.get("prob_boll"),
                                "rsi": sig.get("prob_rsi"),
                                "mom": sig.get("prob_mom"),
                                "vwap": sig.get("prob_vwap"),
                                "orb": sig.get("prob_orb"),
                                "ofi": sig.get("prob_ofi"),
                                "vol": sig.get("prob_vol"),
                                "awr": sig.get("prob_awr"),
                            }
                        },
                        "annotation": ann
                    }
                    publish(producer, prices_topic, sym, payload, {**headers, "timestampET": ts_iso})
            else:
                timestamp_ms = bar_state.get("timestamp_ms")
                ts_iso = bar_state.get("timestamp_et") or iso_et_from_ms(timestamp_ms)
                minute = int(timestamp_ms // 60000 * 60000)
                # Publish prices for all symbols
                for sym in symbols:
                    bar = price_lookup.get(sym, {}).get(minute)
                    if not bar:
                        continue
                    bar_id = bar.get("bar_id") or bar_id_from_ts_iso(ts_iso)
                    payload = {
                        "symbol": sym,
                        "tsET": ts_iso,
                        "barId": bar_id,
                        "open": bar.get("open"),
                        "high": bar.get("high"),
                        "low": bar.get("low"),
                        "close": bar.get("close"),
                        "volume": bar.get("volume")
                    }
                    publish(producer, prices_topic, sym, payload, {**headers, "timestampET": ts_iso})

            # Publish trades that occurred at this timestamp
            for t in trades_by_timestamp.get(timestamp_ms, []):
                sym = t.get("symbol")
                action = t.get("action")
                price = float(t.get("price", 0))
                shares = int(t.get("shares", 0))
                value = float(t.get("value", price * shares))
                pnl = float(t.get("pnl", 0))
                # C++ exports pnl_pct as a percentage number (e.g., -0.52 for -0.52%)
                # Convert to decimal form (e.g., -0.0052) for webapp display
                pnl_pct = float(t.get("pnl_pct", 0)) / 100.0
                reason = t.get("reason", "")
                # Prefer event-specific annotation from engine; fallback to per-bar symbol annotation
                event_ann = t.get("eventAnnotation") or t.get("event_annotation") or ""
                # Sanitize legacy bad text like "target $0 hit" from older runs
                if isinstance(event_ann, str) and ("target $0" in event_ann or "target $0.0" in event_ann):
                    ea = event_ann.replace("target $0.000000 hit; ", "").replace("target $0 hit; ", "").strip()
                    event_ann = ea if ea else "Exit: profit lock/rotation; details unavailable"
                ann_text = event_ann
                if not ann_text and use_golden:
                    # Rebuild annotation map for this bar once (symbol status annotation)
                    try:
                        ann_map = {sa.get("symbol"): sa.get("text", "") for sa in bar_state.get("symbolAnnotations", [])}
                        ann_text = ann_map.get(sym, "")
                    except Exception:
                        ann_text = ""

                # Optional risk levels if present in source
                stop_price = (
                    t.get("stop_price")
                    or t.get("stopPrice")
                )
                target_price = (
                    t.get("target_price")
                    or t.get("targetPrice")
                )
                take_profit_pct = (
                    t.get("take_profit_pct")
                    or t.get("takeProfitPct")
                )
                bar_id_msg = (
                    t.get("bar_id")
                    or bar_state.get("barId")
                    or bar_id_from_ts_iso(ts_iso)
                )

                pub = {
                    "tradeId": str(uuid.uuid4()),
                    "symbol": sym,
                    "action": "BUY" if action == "ENTRY" else "SELL",
                    "tsET": ts_iso,
                    "barId": bar_id_msg,
                    "price": price,
                    "shares": shares,
                    "value": value,
                    "barsHeld": t.get("bars_held", 0),
                    "pnl": pnl,
                    "pnlPct": pnl_pct,
                    "reason": reason,
                    "stopPrice": stop_price,
                    "targetPrice": target_price,
                    "takeProfitPct": take_profit_pct,
                    # eventAnnotation is fixed at trade time; annotation kept for backward compat
                    "eventAnnotation": event_ann or ann_text,
                    "annotation": ann_text
                }
                publish(producer, trades_topic, sym, pub, {**headers, "timestampET": ts_iso})

            # Get current bar positions
            current_positions = set()
            if use_golden:
                current_positions = set(pos.get("symbol") for pos in bar_state.get("positions", []))
            else:
                current_positions = set(pos.get("symbol") for pos in bar_state.get("positions", []))

            # Publish hasPosition=false for positions that closed since last bar
            closed_positions = prev_positions - current_positions
            for symbol in closed_positions:
                close_msg = {
                    "symbol": symbol,
                    "tsET": ts_iso,
                    "barId": bar_state.get("barId") or bar_id_from_ts_iso(ts_iso),
                    "hasPosition": False
                }
                publish(producer, pos_topic, symbol, close_msg, {**headers, "timestampET": ts_iso})

            # Publish positions from bar_state
            for pos in bar_state.get("positions", []):
                symbol = pos.get("symbol")

                # Track entry time for new positions
                if symbol and symbol not in position_entry_times:
                    position_entry_times[symbol] = timestamp_ms or 0

                # Calculate bars held from tracked entry time
                entry_time_ms = position_entry_times.get(symbol, timestamp_ms or 0)
                current_time_ms = timestamp_ms or 0
                bars_held = int((current_time_ms - entry_time_ms) / 60000) if entry_time_ms and current_time_ms else 0

                pos_msg = {
                    "symbol": symbol,
                    "tsET": ts_iso,
                    "barId": bar_state.get("barId") or bar_id_from_ts_iso(ts_iso),
                    "hasPosition": True,
                    "shares": pos.get("shares"),
                    "entryPrice": pos.get("entry_price"),
                    "marketPrice": pos.get("current_price"),
                    "unrealizedPnl": pos.get("unrealized_pnl"),
                    "unrealizedPnlPct": pos.get("unrealized_pnl_pct") / 100.0,  # Convert from percentage
                    "barsHeld": bars_held,
                    "annotation": ann_map.get(symbol, "") if use_golden else ""
                }
                publish(producer, pos_topic, symbol, pos_msg, {**headers, "timestampET": ts_iso})

            # Clean up entry times for closed positions
            for symbol in closed_positions:
                position_entry_times.pop(symbol, None)

            # Update previous positions for next iteration
            prev_positions = current_positions

            # Publish portfolio state from bar_state
            initial_capital = float(results.get("config", {}).get("initial_capital", 100000.0))
            if use_golden:
                equity = bar_state.get("equity")
                cash = bar_state.get("cash")
            else:
                equity = bar_state.get("equity")
                cash = bar_state.get("cash")
            # Handle both camelCase (Golden DB) and snake_case formats
            total_return_pct = bar_state.get("totalReturnPct") or bar_state.get("total_return_pct") or 0.0
            position_count = bar_state.get("positionCount") or bar_state.get("position_count") or 0
            trades_today = bar_state.get("tradesToday") or bar_state.get("trades_today") or 0

            port = {
                "portfolioId": "default",
                "tsET": ts_iso,
                "barId": bar_state.get("barId") or bar_id_from_ts_iso(ts_iso),
                "initialCapital": initial_capital,
                "equity": equity,
                "cash": cash,
                "totalPnl": equity - initial_capital,
                "totalPnlPct": total_return_pct / 100.0,
                "positions": position_count,
                "tradesToday": trades_today
            }
            publish(producer, port_topic, "default", port, {**headers, "timestampET": ts_iso})

            # Publish heartbeat
            hb = {
                "runId": args.run_id,
                "tsET": ts_iso,
                "barId": bar_state.get("barId") or bar_id_from_ts_iso(ts_iso),
                "status": "ok",
            }
            publish(producer, hb_topic, args.run_id, hb, {**headers, "timestampET": ts_iso})

            # Print progress every 50 bars
            if (i + 1) % 50 == 0:
                print(f"[kafka_sidecar] Progress: {i+1}/{len(filtered_bar_data)} bars", flush=True)

            # Sleep to simulate real-time bar arrival
            time.sleep(max(0, args.speed_ms) / 1000.0)

            # Flush with timeout to avoid blocking indefinitely
            producer.flush(timeout=5.0)

        print(f"[kafka_sidecar] Replay completed: {len(filtered_bar_data)} bars streamed", flush=True)
        return

    # polygon mode
    if args.mode == "polygon":
        if WebSocketClient is None:
            raise SystemExit("polygon package not installed. Run: pip install polygon-api-client")

        # Always load symbols from config/symbols.conf (no ENV fallback)
        symbols = load_symbols_from_conf()
        if not symbols:
            raise SystemExit("No symbols found in config/symbols.conf")
        prices_topic = f"{args.prefix}prices.minute.v1"
        hb_topic = f"{args.prefix}heartbeat.v1"
        headers = {"strategy": args.strategy, "env": args.env_name, "runId": args.run_id}
        producer = build_kafka_producer(args.bootstrap)

        # Polygon aggregates (per-minute) via WebSocket v3
        subs = [f"A.{sym}" for sym in symbols]

        def handle_msg(msg: WebSocketMessage):
            try:
                for ev in msg.events:
                    if ev.event_type != "A":
                        continue
                    sym = ev.symbol
                    ts_ms = int(ev.start_timestamp)
                    ts_iso = iso_et_from_ms(ts_ms)
                    payload = {
                        "symbol": sym,
                        "tsET": ts_iso,
                        "open": ev.open,
                        "high": ev.high,
                        "low": ev.low,
                        "close": ev.close,
                        "volume": ev.volume
                    }
                    publish(producer, prices_topic, sym, payload, {**headers, "timestampET": ts_iso})
                    hb = {"runId": args.run_id, "tsET": ts_iso, "status": "ok"}
                    publish(producer, hb_topic, args.run_id, hb, {**headers, "timestampET": ts_iso})
                producer.flush()
            except Exception:
                # Swallow and continue (sidecar should be resilient)
                pass

        ws = WebSocketClient(api_key=args.polygon_key, subscriptions=subs, market="stocks", feed="socket")
        ws.run(handle_msg)
        return


if __name__ == "__main__":
    main()


